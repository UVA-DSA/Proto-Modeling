{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named py_metamap",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-61d1e793ba0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpy_trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpy_metamap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbehaviours\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpy_trees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblackboard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBlackboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named py_metamap"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from NLPutils import NLPutils as NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer # load the stemmer module from NLTK\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import make_scorer\n",
    "import pickle\n",
    "import math\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import time\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import py_trees\n",
    "import py_metamap #**\n",
    "import behaviours as be #** be sure to properly map pymetampap -> try in VM\n",
    "    ### make sure to include behaviours_m.py instead of behaviors.py\n",
    "from py_trees.blackboard import Blackboard\n",
    "from scipy import spatial\n",
    "from pandas import DataFrame\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAAdata(object):\n",
    "    def __init__(self,text,vital,inter):\n",
    "        self.text = text\n",
    "        self.vital = vital\n",
    "        self.inter = inter\n",
    "        \n",
    "def load_RAA_data(path, cv = True): \n",
    "    df = pd.read_excel(path)\n",
    "    interset = set()\n",
    "    interdict = dict()\n",
    "    narratives = df['Narrative']\n",
    "    narratives = [i for i in narratives]\n",
    "    inters = df['Interventions']\n",
    "    vitals = df['Vitals']\n",
    "    vitals = [i for i in vitals]\n",
    "    interventions = []\n",
    "    for item in inters:\n",
    "        inter = item.strip('{}').split('}{')\n",
    "        inter = [i.split(':')[-1].strip().lower() for i in inter]\n",
    "        c_int = []\n",
    "        for j in inter:\n",
    "            interset.add(j)\n",
    "            if j in interdict:\n",
    "                interdict[j] += 1\n",
    "            else:\n",
    "                interdict[j] = 1\n",
    "            c_int.append(j)\n",
    "        interventions.append(c_int)\n",
    "    for inter in list(interdict):\n",
    "        if cv and interdict[inter] < 20: del interdict[inter]\n",
    "    data = [RAAdata(item,vitals[idx],interventions[idx]) for idx,item in enumerate(narratives)]\n",
    "    \n",
    "    return data,interdict\n",
    "\n",
    "def fullmatch(regex, string, flags=0):\n",
    "    \"\"\"Emulate python-3.4 re.fullmatch().\"\"\"\n",
    "    return re.match(\"(?:\" + regex + r\")\\Z\", string, flags=flags)\n",
    "\n",
    "# preprocess utils\n",
    "def cleanHtml(sentence):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', str(sentence))\n",
    "    return cleantext\n",
    "\n",
    "def cleanPunc(sentence):\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "\n",
    "def keepAlpha(sentence):\n",
    "    alpha_sent = \"\"\n",
    "    for word in sentence.split():\n",
    "        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n",
    "        alpha_sent += alpha_word\n",
    "        alpha_sent += \" \"\n",
    "    alpha_sent = alpha_sent.strip()\n",
    "    return alpha_sent\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "\n",
    "def weighted_precision_recall_f1_util (y_test, y_pre, weight = None):\n",
    "    tp, fp, fn = [0. for _ in range(len(y_pre[0]))], [0. for _ in range(len(y_pre[0]))], \\\n",
    "    [0. for _ in range(len(y_pre[0]))]\n",
    "    for idx in range(len(y_pre)):\n",
    "        for i in range(len(y_pre[idx])):\n",
    "            if y_pre[idx][i] == 1 and y_test[idx][i] == 1: tp[i] += 1\n",
    "            elif y_pre[idx][i] == 1 and y_test[idx][i] == 0: fp[i] += 1\n",
    "            elif y_pre[idx][i] == 0 and y_test[idx][i] == 1: fn[i] += 1\n",
    "    precision = [tp[i] / (tp[i] + fp[i]) if tp[i] > 0 or fp[i] > 0 else 0. for i in range(len(tp))]\n",
    "    recall = [tp[i] / (tp[i] + fn[i]) if tp[i] > 0 or fn[i] > 0 else 0. for i in range(len(tp))]\n",
    "    f1 = [2 * precision[i] * recall[i] / (precision[i] + recall[i]) \\\n",
    "         if precision[i] > 0 or recall[i] > 0 else 0. for i in range(len(tp))]\n",
    "    return np.average(precision, weights = weight), np.average(recall, weights = weight), \\\n",
    "np.average(f1, weights = weight)\n",
    "\n",
    "def weighted_precision (y_test, y_pre, weight = None):\n",
    "    precision, _, _ = weighted_precision_recall_f1_util (y_test, y_pre, weight)\n",
    "    return precision\n",
    "\n",
    "def weighted_recall (y_test, y_pre, weight = None):\n",
    "    _, recall, _ = weighted_precision_recall_f1_util (y_test, y_pre, weight)\n",
    "    return recall\n",
    "\n",
    "def weighted_f1 (y_test, y_pre, weight = None):\n",
    "    _, _, f1 = weighted_precision_recall_f1_util (y_test, y_pre, weight)\n",
    "    return f1\n",
    "\n",
    "def show_results(scores):\n",
    "    metrics = ['test_precision_weighted','test_recall_weighted', 'test_f1_weighted',\\\n",
    "            'test_precision_micro', 'test_recall_micro', 'test_f1_micro']\n",
    "    for metric in metrics:\n",
    "        print metric + ':' + '%.2f' % np.average(scores[metric])\n",
    "        \n",
    "def risk_factor(gt, probs, preds):\n",
    "    risk = []\n",
    "    for idx,case in enumerate(probs):\n",
    "        r = 0\n",
    "        for i,prob in enumerate(case):\n",
    "            if preds[idx][i] == 1 and gt[idx][i] == 0:\n",
    "                r += prob * int2fp_score[num2int[i]] / sum(gt[idx])\n",
    "            if preds[idx][i] == 0 and gt[idx][i] == 1:\n",
    "                r += prob * int2fn_score[num2int[i]] / sum(gt[idx])\n",
    "        risk.append(r)\n",
    "    return sum(risk) / len(risk)\n",
    "\n",
    "def trans_prob(probs):\n",
    "    transed_prob = [[0.] * len(probs) for _ in range(len(probs[0]))]\n",
    "    for idx, res in enumerate(probs):\n",
    "        for i, p in enumerate(res):\n",
    "            if len(p) < 2: transed_prob[i][idx] = 1. - p[0]\n",
    "            else: transed_prob[i][idx] = p[1]\n",
    "                \n",
    "    return transed_prob\n",
    "\n",
    "def show_test_results(gt, res, prob, class_weight):\n",
    "    print \"precision_micro\" + ':' + '%.2f' % precision_score(gt, res, average = 'micro')\n",
    "    print \"recall_micro\" + ':' + '%.2f' % recall_score(gt, res, average = 'micro')\n",
    "    print \"f1_micro\" + ':' + '%.2f' % f1_score(gt, res, average = 'micro')\n",
    "    print \"precision_weighted\" + ':' + '%.2f' % weighted_precision(gt, res, class_weight)\n",
    "    print \"recall_weighted\" + ':' + '%.2f' % weighted_recall(gt, res, class_weight)\n",
    "    print \"f1_weighted\" + ':' + '%.2f' % weighted_f1(gt, res, class_weight)\n",
    "    print \"risk_factor\" + ':' + '%.4f' % risk_factor(gt, prob, res)\n",
    "    \n",
    "def filtering(res, prob, threshold):\n",
    "    for idx, case in enumerate(res):\n",
    "        for i in range(len(case)):\n",
    "            if prob[idx][i] < threshold:\n",
    "                res[idx][i] = 0\n",
    "                prob[idx][i] = 0.\n",
    "    return res, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read labeled cases\n",
    "docu = './RAA_train.xlsx'\n",
    "df = pd.read_excel(docu)\n",
    "train_narratives = df['Narrative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Declare RAA data object \n",
    "#then call your object functions corectly for vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'be' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-abfcdb6f5c5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mblackboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpy_trees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomposites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Root_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mIG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInformationGathering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0mTC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextCollection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'be' is not defined"
     ]
    }
   ],
   "source": [
    "##SKIP##\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "EKGset = set()\n",
    "train_vec = []\n",
    "EKGdic = {\n",
    "     '':'',\n",
    "     'AV_Block_1st_Deg':'AV_Block-1st_Degree',\n",
    "     'AV_Block_1st_Degree':'AV_Block-1st_Degree',\n",
    "     'AV_Block_2nd_Degree_Type_1':'AV_Block_2nd_Degree_Type_1',\n",
    "     'AV_Block_2nd_Degree_Type_2':'AV_Block_2nd_Degree_Type_2',\n",
    "     'AV_Block_3rd_Degree':'AV_Block_3rd_Degree',\n",
    "     'Asystole':'Asystole',\n",
    "     'Artifact':'Artifact',\n",
    "     'Atrial_Fibrill':'Atrial_Fibrillation',\n",
    "     'Atrial_Fibrillation':'Atrial_Fibrillation',\n",
    "     'Atrial_Flutter':'Atrial_Flutter',\n",
    "     'Agonal/Idioventricular':'Agonal/Idioventricular',\n",
    "     'Juncti':'Junctional_Rhythm',\n",
    "     'Junctiona':'Junctional_Rhythm',\n",
    "     'Junctional':'Junctional_Rhythm',\n",
    "     'Non_STEMI_Anterior_Ischemia':'Non_STEMI_Anterior_Ischemia',\n",
    "     'Non_STEMI_Lateral_Ischemia':'Non_STEMI_Lateral_Ischemia',\n",
    "     'Other_(Not_Listed)':'Other_(Not_Listed)',\n",
    "     'P': 'Paced_Rhythm',\n",
    "     'PEA':'Pulseless_Electrical_Activity',\n",
    "     'Pac':'Paced_Rhythm',\n",
    "     'Paced':'Paced_Rhythm',\n",
    "     'Paced_Rhythm':'Paced_Rhythm',\n",
    "     'Premature_Ventricular_Contractions':'Premature_Ventricular_Contractions',\n",
    "     'Premature_Atrial_Contractions':'Premature_Atrial_Contractions',\n",
    "     'Right_Bundle_Branch_Block':'Right_Bundle_Branch_Block',\n",
    "     'Left_Bundle_Branch_Block':'Left_Bundle_Branch_Block',\n",
    "     'STEMI_Anterior_Ischemia':'STEMI_Anterior_Ischemia',\n",
    "     'STEMI_Lateral_Ischemia':'STEMI_Lateral_Ischemia',\n",
    "     'STEMI_Inferior_Ischemia':'STEMI_Inferior_Ischemia',\n",
    "     'S':'Sinus_Rhythm',\n",
    "     'Si':'Sinus_Rhythm',\n",
    "     'Sin':'Sinus_Rhythm',\n",
    "     'Sinu':'Sinus_Rhythm',\n",
    "     'Sinus':'Sinus_Rhythm',\n",
    "     'Sinus_':'Sinus_Rhythm',\n",
    "     'Sinus_Arrhythmia':'Sinus_Arrhythmia',\n",
    "     'Sinus_Bradycardia':'Sinus_Bradycardia',\n",
    "     'Sinus_R':'Sinus_Rhythm',\n",
    "     'Sinus_Rh':'Sinus_Rhythm',\n",
    "     'Sinus_Rhy':'Sinus_Rhythm',\n",
    "     'Sinus_Rhyt':'Sinus_Rhythm',\n",
    "     'Sinus_Rhyth':'Sinus_Rhythm',\n",
    "     'Sinus_Rhythm':'Sinus_Rhythm',\n",
    "     'Sinus_Rhythm,Sinus_Tachycardia':'Sinus_Rhythm,Sinus_Tachycardia',\n",
    "     'Sinus_T':'Sinus_Tachycardia',\n",
    "     'Sinus_Tach':'Sinus_Tachycardia',\n",
    "     'Sinus_Tachyc':'Sinus_Tachycardia',\n",
    "     'Sinus_Tachycardi':'Sinus_Tachycardia',\n",
    "     'Sinus_Tachycardia':'Sinus_Tachycardia',\n",
    "     'Supravent':'Supraventricular_Tachycardia',\n",
    "     'Supraventricular_Tachycardia':'Supraventricular_Tachycardia',\n",
    "     'Torsades_De_Points':'Torsades_De_Points',\n",
    "     'Ventricular_Fibrillation':'Ventricular_Fibrillation',\n",
    "     'Ventricular_Tachycardia_(With_Pulse)':'Ventricular_Tachycardia',\n",
    "     'Ventricular_Tachycardia_(Pulseless)':'Ventricular_Tachycardia_(Pulseless)',\n",
    "     'Unknown_AED_Non_Shockable_Rhythm':'Unknown_AED_Non_Shockable_Rhythm'\n",
    "    \n",
    "}\n",
    "# extract concept and calculate similarity\n",
    "from ranking_func import rank\n",
    "pool = set(['Medical - Abdominal Pain',\n",
    "            'Medical - Altered Mental Status',\n",
    "            'Medical - Seizure',\n",
    "            'Medical - Respiratory Distress/Asthma/COPD/Croup/Reactive Airway',\n",
    "            'General - Behavioral/Patient Restraint',\n",
    "            'Medical - Overdose/Poisoning - Opioid',\n",
    "            'Medical - Diabetic - Hypoglycemia',\n",
    "            'Medical - Chest Pain - Cardiac Suspected'])\n",
    "\n",
    "def pre_tick_handler(behaviour_tree):\n",
    "    blackboard = Blackboard()\n",
    "    blackboard.tick_num += 1\n",
    "    \n",
    "pt = 0\n",
    "vitals = []\n",
    "\n",
    "#for i,item in enumerate(tqdm(train_narratives)): *** Update jupyter notebook ***\n",
    "for i,item in enumerate((train_narratives)):\n",
    "    if (vitals == None) :\n",
    "        if not pd.isnull(vitals[i]):\n",
    "            vt = vitals[i].strip('{}').split('}{')\n",
    "            vt = [it.split(':')[-1] for it in vt]\n",
    "            for idx,it in enumerate(vt):\n",
    "                if 'EKG-' in it:\n",
    "                    temp = it.split('EKG-')\n",
    "                    temp[1] = temp[1].replace(' ','_')\n",
    "                    temp[1] = temp[1].replace('-','_')\n",
    "                    if ',' in temp[1]:\n",
    "                        t = temp[1].split(',')\n",
    "                        t = [EKGdic[i] for i in t]\n",
    "                        vt[idx] = temp[0] + 'EKG-' + ','.join(t)\n",
    "                    else:\n",
    "                        vt[idx] = temp[0] + 'EKG-' + EKGdic[temp[1]]\n",
    "                    if len(temp[1]) > 0:\n",
    "                        EKGset.add(temp[1])\n",
    "            vt = [ite for l in vt for ite in l.strip().split(' ')]\n",
    "            for idx in xrange(len(vt)):\n",
    "                if idx < len(vt) and '-' not in vt[idx]:\n",
    "                    vt.pop(idx)\n",
    "            for idx,it in enumerate(vt):\n",
    "                temp = it.split('-')\n",
    "                vt[idx] = (temp[0],temp[1])\n",
    "    blackboard = Blackboard()\n",
    "    blackboard.text = [item]\n",
    "    root = py_trees.composites.Sequence(\"Root_1\")\n",
    "    IG = be.InformationGathering(inC = vt)\n",
    "    TC = be.TextCollection()\n",
    "    V = be.Vectorize()\n",
    "    root.add_children([TC,IG,V])\n",
    "    behaviour_tree = py_trees.trees.BehaviourTree(root)\n",
    "    behaviour_tree.add_pre_tick_handler(pre_tick_handler)\n",
    "    behaviour_tree.setup(15)\n",
    "    behaviour_tree.tick_tock(\n",
    "            sleep_ms=50,\n",
    "            number_of_iterations=1,\n",
    "            pre_tick_handler=None,\n",
    "       post_tick_handler=None\n",
    "    )\n",
    "    pt = i\n",
    "    train_vec.append(blackboard.TV)\n",
    "    with open('train_vec.txt', 'w') as f:\n",
    "        pickle.dump(blackboard.TV, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9226f83200fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##SKIP##with open('train_vec_list.txr','w') as fo:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fo' is not defined"
     ]
    }
   ],
   "source": [
    "##SKIP##\n",
    "with open('train_vec_list.txr','w') as fo:\n",
    "pickle.dump(train_vec, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SKIP##\n",
    "with open('train_vec_list.txr') as fo:\n",
    "train_vec = pickle.load(fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "docu = './RAA_1000_test.xlsx'\n",
    "df = pd.read_excel(docu)\n",
    "test_narratives = df['Narrative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_narratives' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1ab4eff170f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0mpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_narratives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvitals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mvt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvitals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'}{'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_narratives' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "EKGset = set()\n",
    "test_vec = []\n",
    "EKGdic = {\n",
    "     '':'',\n",
    "     'AV_Block_1st_Deg':'AV_Block-1st_Degree',\n",
    "     'AV_Block_1st_Degree':'AV_Block-1st_Degree',\n",
    "     'AV_Block_2nd_Degree_Type_1':'AV_Block_2nd_Degree_Type_1',\n",
    "     'AV_Block_2nd_Degree_Type_2':'AV_Block_2nd_Degree_Type_2',\n",
    "     'AV_Block_3rd_Degree':'AV_Block_3rd_Degree',\n",
    "     'Asystole':'Asystole',\n",
    "     'Artifact':'Artifact',\n",
    "     'Atrial_Fibrill':'Atrial_Fibrillation',\n",
    "     'Atrial_Fibrillation':'Atrial_Fibrillation',\n",
    "     'Atrial_Flutter':'Atrial_Flutter',\n",
    "     'Agonal/Idioventricular':'Agonal/Idioventricular',\n",
    "     'Juncti':'Junctional_Rhythm',\n",
    "     'Junctiona':'Junctional_Rhythm',\n",
    "     'Junctional':'Junctional_Rhythm',\n",
    "     'Non_STEMI_Anterior_Ischemia':'Non_STEMI_Anterior_Ischemia',\n",
    "     'Non_STEMI_Lateral_Ischemia':'Non_STEMI_Lateral_Ischemia',\n",
    "     'Other_(Not_Listed)':'Other_(Not_Listed)',\n",
    "     'P': 'Paced_Rhythm',\n",
    "     'PEA':'Pulseless_Electrical_Activity',\n",
    "     'Pac':'Paced_Rhythm',\n",
    "     'Paced':'Paced_Rhythm',\n",
    "     'Paced_Rhythm':'Paced_Rhythm',\n",
    "     'Premature_Ventricular_Contractions':'Premature_Ventricular_Contractions',\n",
    "     'Premature_Atrial_Contractions':'Premature_Atrial_Contractions',\n",
    "     'Right_Bundle_Branch_Block':'Right_Bundle_Branch_Block',\n",
    "     'Left_Bundle_Branch_Block':'Left_Bundle_Branch_Block',\n",
    "     'STEMI_Anterior_Ischemia':'STEMI_Anterior_Ischemia',\n",
    "     'STEMI_Lateral_Ischemia':'STEMI_Lateral_Ischemia',\n",
    "     'STEMI_Inferior_Ischemia':'STEMI_Inferior_Ischemia',\n",
    "     'S':'Sinus_Rhythm',\n",
    "     'Si':'Sinus_Rhythm',\n",
    "     'Sin':'Sinus_Rhythm',\n",
    "     'Sinu':'Sinus_Rhythm',\n",
    "     'Sinus':'Sinus_Rhythm',\n",
    "     'Sinus_':'Sinus_Rhythm',\n",
    "     'Sinus_Arrhythmia':'Sinus_Arrhythmia',\n",
    "     'Sinus_Bradycardia':'Sinus_Bradycardia',\n",
    "     'Sinus_R':'Sinus_Rhythm',\n",
    "     'Sinus_Rh':'Sinus_Rhythm',\n",
    "     'Sinus_Rhy':'Sinus_Rhythm',\n",
    "     'Sinus_Rhyt':'Sinus_Rhythm',\n",
    "     'Sinus_Rhyth':'Sinus_Rhythm',\n",
    "     'Sinus_Rhythm':'Sinus_Rhythm',\n",
    "     'Sinus_Rhythm,Sinus_Tachycardia':'Sinus_Rhythm,Sinus_Tachycardia',\n",
    "     'Sinus_T':'Sinus_Tachycardia',\n",
    "     'Sinus_Tach':'Sinus_Tachycardia',\n",
    "     'Sinus_Tachyc':'Sinus_Tachycardia',\n",
    "     'Sinus_Tachycardi':'Sinus_Tachycardia',\n",
    "     'Sinus_Tachycardia':'Sinus_Tachycardia',\n",
    "     'Supravent':'Supraventricular_Tachycardia',\n",
    "     'Supraventricular_Tachycardia':'Supraventricular_Tachycardia',\n",
    "     'Torsades_De_Points':'Torsades_De_Points',\n",
    "     'Ventricular_Fibrillation':'Ventricular_Fibrillation',\n",
    "     'Ventricular_Tachycardia_(With_Pulse)':'Ventricular_Tachycardia',\n",
    "     'Ventricular_Tachycardia_(Pulseless)':'Ventricular_Tachycardia_(Pulseless)',\n",
    "     'Unknown_AED_Non_Shockable_Rhythm':'Unknown_AED_Non_Shockable_Rhythm'\n",
    "    \n",
    "}\n",
    "# extract concept and calculate similarity\n",
    "from ranking_func import rank\n",
    "pool = set(['Medical - Abdominal Pain',\n",
    "            'Medical - Altered Mental Status',\n",
    "            'Medical - Seizure',\n",
    "            'Medical - Respiratory Distress/Asthma/COPD/Croup/Reactive Airway',\n",
    "            'General - Behavioral/Patient Restraint',\n",
    "            'Medical - Overdose/Poisoning - Opioid',\n",
    "            'Medical - Diabetic - Hypoglycemia',\n",
    "            'Medical - Chest Pain - Cardiac Suspected'])\n",
    "\n",
    "def pre_tick_handler(behaviour_tree):\n",
    "    blackboard = Blackboard()\n",
    "    blackboard.tick_num += 1\n",
    "    \n",
    "pt = 0\n",
    "vitals = []\n",
    "\n",
    "#for i,item in enumerate(tqdm(train_narratives)): *** Update jupyter notebook ***\n",
    "for i,item in enumerate((test_narratives)):\n",
    "    if (vitals == None) :\n",
    "        if not pd.isnull(vitals[i]):\n",
    "            vt = vitals[i].strip('{}').split('}{')\n",
    "            vt = [it.split(':')[-1] for it in vt]\n",
    "            for idx,it in enumerate(vt):\n",
    "                if 'EKG-' in it:\n",
    "                    temp = it.split('EKG-')\n",
    "                    temp[1] = temp[1].replace(' ','_')\n",
    "                    temp[1] = temp[1].replace('-','_')\n",
    "                    if ',' in temp[1]:\n",
    "                        t = temp[1].split(',')\n",
    "                        t = [EKGdic[i] for i in t]\n",
    "                        vt[idx] = temp[0] + 'EKG-' + ','.join(t)\n",
    "                    else:\n",
    "                        vt[idx] = temp[0] + 'EKG-' + EKGdic[temp[1]]\n",
    "                    if len(temp[1]) > 0:\n",
    "                        EKGset.add(temp[1])\n",
    "            vt = [ite for l in vt for ite in l.strip().split(' ')]\n",
    "            for idx in xrange(len(vt)):\n",
    "                if idx < len(vt) and '-' not in vt[idx]:\n",
    "                    vt.pop(idx)\n",
    "            for idx,it in enumerate(vt):\n",
    "                temp = it.split('-')\n",
    "                vt[idx] = (temp[0],temp[1])\n",
    "    blackboard = Blackboard()\n",
    "    blackboard.text = [item]\n",
    "    root = py_trees.composites.Sequence(\"Root_1\")\n",
    "    IG = be.InformationGathering(inC = vt)\n",
    "    TC = be.TextCollection()\n",
    "    V = be.Vectorize()\n",
    "    root.add_children([TC,IG,V])\n",
    "    behaviour_tree = py_trees.trees.BehaviourTree(root)\n",
    "    behaviour_tree.add_pre_tick_handler(pre_tick_handler)\n",
    "    behaviour_tree.setup(15)\n",
    "    behaviour_tree.tick_tock(\n",
    "            sleep_ms=50,\n",
    "            number_of_iterations=1,\n",
    "            pre_tick_handler=None,\n",
    "       post_tick_handler=None\n",
    "    )\n",
    "    pt = i\n",
    "    test_vec.append(blackboard.TV)\n",
    "    with open('test_vec.txt', 'w') as f:\n",
    "        pickle.dump(blackboard.TV, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('test_vec_list.txr','w') as fo:\n",
    "    pickle.dump(test_vec, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('test_vec_list.txr') as fo:\n",
    "    test_vec = pickle.load(fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/Users/sileshu/Desktop/EMSdata/RAA_train.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-228c011ca843>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnarra\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mintdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_RAA_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/sileshu/Desktop/EMSdata/RAA_train.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_narra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_RAA_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/sileshu/Desktop/EMSdata/RAA_1000_test.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrisk_route\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/sileshu/Desktop/EMSData/Intervention Safety Sheet.xlsx'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_risk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrisk_route\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mint2fn_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-5201d5d9af1f>\u001b[0m in \u001b[0;36mload_RAA_data\u001b[0;34m(path, cv)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_RAA_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0minterset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0minterdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lidolla/anaconda2/lib/python2.7/site-packages/pandas/util/_decorators.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lidolla/anaconda2/lib/python2.7/site-packages/pandas/util/_decorators.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lidolla/anaconda2/lib/python2.7/site-packages/pandas/io/excel.pyc\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     return io.parse(\n",
      "\u001b[0;32m/home/lidolla/anaconda2/lib/python2.7/site-packages/pandas/io/excel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, io, engine)\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_stringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lidolla/anaconda2/lib/python2.7/site-packages/pandas/io/excel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             raise ValueError('Must explicitly set engine if not passing in'\n",
      "\u001b[0;32m/home/lidolla/anaconda2/lib/python2.7/site-packages/xlrd/__init__.pyc\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/Users/sileshu/Desktop/EMSdata/RAA_train.xlsx'"
     ]
    }
   ],
   "source": [
    "narra,intdict = load_RAA_data('./RAA_train.xlsx')\n",
    "test_narra, _ = load_RAA_data('./RAA_1000_test.xlsx', cv = False)\n",
    "risk_route = './Intervention Safety Sheet.xlsx'\n",
    "df_risk = pd.read_excel(risk_route)\n",
    "int2fn_score = dict()\n",
    "int2fp_score = dict()\n",
    "for row in df_risk.iterrows():\n",
    "    name = row[1]['Intervention'].split('\\'')[1]\n",
    "    FN_score, FP_score = 0, 0\n",
    "    if not pd.isnull(row[1]['If NOT Done When Indicated']):\n",
    "        FN_score = int(row[1]['If NOT Done When Indicated'])\n",
    "    if not pd.isnull(row[1]['If Done When NOT Indicated']):\n",
    "        FP_score = int(row[1]['If Done When NOT Indicated'])\n",
    "    if not FN_score or not FP_score or (name not in intdict):\n",
    "        continue\n",
    "    int2fn_score[name] = FN_score\n",
    "    int2fp_score[name] = FP_score\n",
    "int2num = dict()\n",
    "num2int = dict()\n",
    "for i,key in enumerate(int2fn_score):\n",
    "    int2num[key] = i\n",
    "    num2int[i] = key\n",
    "#n = NLP()\n",
    "# load technical n-grams\n",
    "fo = open('ngrams.txt')\n",
    "ngrams = set()\n",
    "for line in fo:\n",
    "    if line == '\\n': continue\n",
    "    ngrams.add(line.strip('\\n'))\n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inter_safety = [dict() for _ in range(len(int2num))]\n",
    "for idx in range(len(num2int)):\n",
    "    inter_safety[idx][0] = 1. / int2fn_score[num2int[idx]]\n",
    "    inter_safety[idx][1] = 1. / int2fp_score[num2int[idx]]\n",
    "inter_safety_dic = dict()\n",
    "for idx in range(len(num2int)):\n",
    "    inter_safety_dic[idx] = int2fp_score[num2int[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_text = [i.text for i in narra]\n",
    "total_inter = [i.inter for i in narra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train,_ = train_test_split(narra, random_state=46, test_size=.2, shuffle=True)\n",
    "train_text = [i.text for i in train]\n",
    "train_inter = [i.inter for i in train]\n",
    "test_text = [i.text for i in test_narra]\n",
    "test_inter = [i.inter for i in test_narra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sileshu/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [u'abov', u'afterward', u'alon', u'alreadi', u'alway', u'ani', u'anoth', u'anyon', u'anyth', u'anywher', u'becam', u'becaus', u'becom', u'befor', u'besid', u'cri', u'describ', u'dure', u'els', u'elsewher', u'empti', u'everi', u'everyon', u'everyth', u'everywher', u'fifti', u'forti', u'henc', u'hereaft', u'herebi', u'howev', u'hundr', u'inde', u'mani', u'meanwhil', u'moreov', u'nobodi', u'noon', u'noth', u'nowher', u'onc', u'onli', u'otherwis', u'ourselv', u'perhap', u'pleas', u'sever', u'sinc', u'sincer', u'sixti', u'someon', u'someth', u'sometim', u'somewher', u'themselv', u'thenc', u'thereaft', u'therebi', u'therefor', u'togeth', u'twelv', u'twenti', u'veri', u'whatev', u'whenc', u'whenev', u'wherea', u'whereaft', u'wherebi', u'wherev', u'whi', u'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = cleanPunc(text)\n",
    "    text = keepAlpha(text)\n",
    "    text = stemming(text)\n",
    "    return text\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1), preprocessor = preprocess, stop_words = 'english', norm='l2')\n",
    "vectorizer.fit(train_text)\n",
    "x_train = vectorizer.transform(train_text)\n",
    "y_train = [[int2num[inter] for inter in case if inter in int2num] for case in train_inter]\n",
    "encoded_y_train = np.array([[int(num in case) for num in range(len(int2num))] for case in y_train])\n",
    "x_test = vectorizer.transform(test_text)\n",
    "y_test = [[int2num[inter] for inter in case if inter in int2num] for case in test_inter]\n",
    "encoded_y_test = np.array([[int(num in case) for num in range(len(int2num))] for case in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9846d79e3946>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint2num\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minter\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mint2num\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtotal_inter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mencoded_y_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint2num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_total\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocess' is not defined"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,1), preprocessor = preprocess, stop_words = 'english', norm='l2')\n",
    "vectorizer.fit(total_text)\n",
    "x_total = vectorizer.transform(total_text)\n",
    "y_total = [[int2num[inter] for inter in case if inter in int2num] for case in total_inter]\n",
    "encoded_y_total = np.array([[int(num in case) for num in range(len(int2num))] for case in y_total])\n",
    "class_weight = np.sum(encoded_y_total, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,4),vocabulary = ngrams,\\\n",
    "                             preprocessor = preprocess, stop_words = 'english', norm='l2')\n",
    "vectorizer.fit(train_text)\n",
    "x_train_ngram = vectorizer.transform(train_text)\n",
    "y_train = [[int2num[inter] for inter in case if inter in int2num] for case in train_inter]\n",
    "encoded_y_train = np.array([[int(num in case) for num in range(len(int2num))] for case in y_train])\n",
    "x_test_ngram = vectorizer.transform(test_text)\n",
    "y_test = [[int2num[inter] for inter in case if inter in int2num] for case in test_inter]\n",
    "encoded_y_test = np.array([[int(num in case) for num in range(len(int2num))] for case in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,4),vocabulary = ngrams,\\\n",
    "                             preprocessor = preprocess, stop_words = 'english', norm='l2')\n",
    "vectorizer.fit(total_text)\n",
    "x_total_ngram = vectorizer.transform(total_text)\n",
    "y_total = [[int2num[inter] for inter in case if inter in int2num] for case in total_inter]\n",
    "encoded_y_total = np.array([[int(num in case) for num in range(len(int2num))] for case in y_total])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoring = {'precision_weighted': make_scorer(weighted_precision, weight = class_weight),\n",
    "           'recall_weighted': make_scorer(weighted_recall, weight = class_weight),\n",
    "           'f1_weighted': make_scorer(weighted_f1, weight = class_weight),\n",
    "           'precision_micro': 'precision_micro',\n",
    "           'recall_micro': 'recall_micro',\n",
    "           'f1_micro': 'f1_micro'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_weighted:0.89\n",
      "test_recall_weighted:0.83\n",
      "test_f1_weighted:0.85\n",
      "test_precision_micro:0.92\n",
      "test_recall_micro:0.84\n",
      "test_f1_micro:0.88\n"
     ]
    }
   ],
   "source": [
    "clf = OneVsRestClassifier(SVC(kernel = 'linear', probability=True))\n",
    "scores_1 = cross_validate(clf, x_total, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)\n",
    "show_results(scores_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.9412081242\n",
      "precision_micro:0.92\n",
      "recall_micro:0.88\n",
      "f1_micro:0.90\n",
      "precision_weighted:0.88\n",
      "recall_weighted:0.86\n",
      "f1_weighted:0.87\n",
      "risk_factor:0.2360\n"
     ]
    }
   ],
   "source": [
    "clf = OneVsRestClassifier(SVC(kernel = 'linear', probability=True))\n",
    "clf.fit(x_train, encoded_y_train)\n",
    "start_time = time.time()\n",
    "y_pre_1 = clf.predict(x_test)\n",
    "elapsed_time = time.time() - start_time\n",
    "print elapsed_time\n",
    "y_pos_1 = clf.predict_proba(x_test)\n",
    "show_test_results(encoded_y_test, y_pre_1, y_pos_1, class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_rf = RandomForestClassifier()\n",
    "scores_2 = cross_validate(clf_rf, x_total, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_weighted:0.86\n",
      "test_recall_weighted:0.70\n",
      "test_f1_weighted:0.72\n",
      "test_precision_micro:0.89\n",
      "test_recall_micro:0.69\n",
      "test_f1_micro:0.78\n"
     ]
    }
   ],
   "source": [
    "show_results(scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sileshu/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.104833841324\n",
      "precision_micro:0.90\n",
      "recall_micro:0.71\n",
      "f1_micro:0.79\n",
      "precision_weighted:0.79\n",
      "recall_weighted:0.68\n",
      "f1_weighted:0.70\n",
      "risk_factor:0.4160\n"
     ]
    }
   ],
   "source": [
    "clf_rf = RandomForestClassifier()\n",
    "clf_rf.fit(x_train, encoded_y_train)\n",
    "start_time = time.time()\n",
    "y_pre_2 = clf_rf.predict(x_test)\n",
    "elapsed_time = time.time() - start_time\n",
    "print elapsed_time\n",
    "y_pos_2 = clf_rf.predict_proba(x_test)\n",
    "show_test_results(encoded_y_test, y_pre_2, trans_prob(y_pos_2), class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_dt = DecisionTreeClassifier()\n",
    "scores_3 = cross_validate(clf_dt, x_total, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_weighted:0.79\n",
      "test_recall_weighted:0.81\n",
      "test_f1_weighted:0.79\n",
      "test_precision_micro:0.81\n",
      "test_recall_micro:0.81\n",
      "test_f1_micro:0.81\n"
     ]
    }
   ],
   "source": [
    "show_results(scores_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00572490692139\n",
      "precision_micro:0.85\n",
      "recall_micro:0.82\n",
      "f1_micro:0.83\n",
      "precision_weighted:0.82\n",
      "recall_weighted:0.80\n",
      "f1_weighted:0.81\n",
      "risk_factor:0.2600\n"
     ]
    }
   ],
   "source": [
    "clf_dt = DecisionTreeClassifier()\n",
    "clf_dt.fit(x_train, encoded_y_train)\n",
    "start_time = time.time()\n",
    "y_pre_3 = clf_dt.predict(x_test)\n",
    "elapsed_time = time.time() - start_time\n",
    "print elapsed_time\n",
    "y_pos_3 = clf_dt.predict_proba(x_test)\n",
    "show_test_results(encoded_y_test, y_pre_3, trans_prob(y_pos_3), class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bvm\n",
      "0.0676099916781\n",
      "lead\n",
      "0.0615468256244\n",
      "albuterol\n",
      "0.0523698815699\n",
      "iv\n",
      "0.0429005333391\n",
      "cpr\n",
      "0.0340569427189\n",
      "zofran\n",
      "0.0319218765782\n",
      "chest\n",
      "0.0291627825644\n",
      "fentanyl\n",
      "0.022911408512\n",
      "refus\n",
      "0.0178968271152\n",
      "asa\n",
      "0.0166608628015\n",
      "narcan\n",
      "0.0144680535039\n",
      "cpap\n",
      "0.0137906317306\n",
      "vers\n",
      "0.0128401598771\n",
      "wheez\n",
      "0.0103049422182\n",
      "glucos\n",
      "0.00861773685179\n",
      "arrest\n",
      "0.00794210243553\n",
      "neb\n",
      "0.00765488523002\n",
      "salin\n",
      "0.0075706057816\n",
      "lpm\n",
      "0.00727740968704\n",
      "io\n",
      "0.00714153514721\n",
      "duoneb\n",
      "0.0067604377549\n",
      "oxygen\n",
      "0.00654877312823\n",
      "dexamethason\n",
      "0.00557503260953\n",
      "oral\n",
      "0.00540932356188\n",
      "cardiac\n",
      "0.00520209919327\n",
      "ondansetron\n",
      "0.0051942454993\n",
      "reaction\n",
      "0.00431170784098\n",
      "patient\n",
      "0.0042182957055\n",
      "monitor\n",
      "0.00401754668154\n",
      "assess\n",
      "0.00389954624188\n"
     ]
    }
   ],
   "source": [
    "heap = []\n",
    "for idx, importance in enumerate(clf_dt.feature_importances_):\n",
    "    heap.append((importance, idx))\n",
    "heap.sort(reverse = True)\n",
    "for item in heap[:30]:\n",
    "    print vectorizer.get_feature_names()[item[1]]\n",
    "    print item[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_4 = OneVsRestClassifier(SVC(kernel = 'linear', probability=True))\n",
    "scores_4 = cross_validate(clf_4, x_total_ngram, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_weighted:0.82\n",
      "test_recall_weighted:0.75\n",
      "test_f1_weighted:0.76\n",
      "test_precision_micro:0.88\n",
      "test_recall_micro:0.75\n",
      "test_f1_micro:0.81\n"
     ]
    }
   ],
   "source": [
    "show_results(scores_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.89\n",
      "recall_micro:0.77\n",
      "f1_micro:0.83\n",
      "precision_weighted:0.83\n",
      "recall_weighted:0.77\n",
      "f1_weighted:0.78\n",
      "risk_factor:0.3177\n"
     ]
    }
   ],
   "source": [
    "clf_4 = OneVsRestClassifier(SVC(kernel = 'linear', probability=True))\n",
    "clf_4.fit(x_train_ngram, encoded_y_train)\n",
    "y_pre_4 = clf_4.predict(x_test_ngram)\n",
    "y_pos_4 = clf_4.predict_proba(x_test_ngram)\n",
    "show_test_results(encoded_y_test, y_pre_4, y_pos_4, class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_rf_1 = RandomForestClassifier()\n",
    "scores_5 = cross_validate(clf_rf_1, x_total_ngram, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_weighted:0.83\n",
      "test_recall_weighted:0.66\n",
      "test_f1_weighted:0.69\n",
      "test_precision_micro:0.88\n",
      "test_recall_micro:0.66\n",
      "test_f1_micro:0.75\n"
     ]
    }
   ],
   "source": [
    "show_results(scores_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.88\n",
      "recall_micro:0.66\n",
      "f1_micro:0.76\n",
      "precision_weighted:0.82\n",
      "recall_weighted:0.67\n",
      "f1_weighted:0.69\n",
      "risk_factor:0.4570\n"
     ]
    }
   ],
   "source": [
    "clf_rf_1 = RandomForestClassifier()\n",
    "clf_rf_1.fit(x_train_ngram,encoded_y_train)\n",
    "y_pre_5 = clf_rf_1.predict(x_test_ngram)\n",
    "y_pos_5 = clf_rf_1.predict_proba(x_test_ngram)\n",
    "show_test_results(encoded_y_test, y_pre_5, trans_prob(y_pos_5), class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.03353867214\n"
     ]
    }
   ],
   "source": [
    "y_pos_5_c = [[0.] * len(y_pos_5) for _ in range(len(y_pos_5[0]))]\n",
    "for idx, res in enumerate(y_pos_5):\n",
    "    for i, p in enumerate(res):\n",
    "        if len(p) < 2: y_pos_5_c[i][idx] = 1. - p[0]\n",
    "        else: y_pos_5_c[i][idx] = p[1]\n",
    "\n",
    "risk = []\n",
    "for case in y_pos_5_c:\n",
    "    r = 0\n",
    "    for i,pos in enumerate(case):\n",
    "        r += pos * num2risk[num2int[i]]\n",
    "    risk.append(r)\n",
    "print sum(risk)/len(risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_dt_1 = DecisionTreeClassifier()\n",
    "scores_6 = cross_validate(clf_dt_1, x_total_ngram, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_weighted:0.71\n",
      "test_recall_weighted:0.72\n",
      "test_f1_weighted:0.71\n",
      "test_precision_micro:0.75\n",
      "test_recall_micro:0.73\n",
      "test_f1_micro:0.74\n"
     ]
    }
   ],
   "source": [
    "show_results(scores_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.78\n",
      "recall_micro:0.74\n",
      "f1_micro:0.76\n",
      "precision_weighted:0.73\n",
      "recall_weighted:0.71\n",
      "f1_weighted:0.72\n",
      "risk_factor:0.4505\n"
     ]
    }
   ],
   "source": [
    "clf_dt_1 = DecisionTreeClassifier()\n",
    "clf_dt_1.fit(x_train_ngram,encoded_y_train)\n",
    "y_pre_6 = clf_dt_1.predict(x_test_ngram)\n",
    "y_pos_6 = clf_dt_1.predict_proba(x_test_ngram)\n",
    "\n",
    "show_test_results(encoded_y_test, y_pre_6, trans_prob(y_pos_6), class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.92539356605\n"
     ]
    }
   ],
   "source": [
    "y_pos_6_c = [[0.] * len(y_pos_6) for _ in range(len(y_pos_6[0]))]\n",
    "for idx, res in enumerate(y_pos_6):\n",
    "    for i, p in enumerate(res):\n",
    "        if len(p) < 2: y_pos_6_c[i][idx] = 1. - p[0]\n",
    "        else: y_pos_6_c[i][idx] = p[1]\n",
    "\n",
    "risk = []\n",
    "for case in y_pos_6_c:\n",
    "    r = 0\n",
    "    for i,pos in enumerate(case):\n",
    "        r += pos * num2risk[num2int[i]]\n",
    "    risk.append(r)\n",
    "print sum(risk)/len(risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Class label 2 not present.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-7b8461f3988f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclf_7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneVsRestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minter_safety_dic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# cv_results = cross_validate(clf, x_train, y_train, cv=5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclf_7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoded_y_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_pre_7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pos_7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sileshu/anaconda/lib/python2.7/site-packages/sklearn/multiclass.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;34m\"not %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_binarizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 self.label_binarizer_.classes_[i]])\n\u001b[0;32m--> 215\u001b[0;31m             for i, column in enumerate(columns))\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sileshu/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sileshu/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sileshu/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sileshu/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sileshu/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sileshu/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sileshu/anaconda/lib/python2.7/site-packages/sklearn/multiclass.pyc\u001b[0m in \u001b[0;36m_fit_binary\u001b[0;34m(estimator, X, y, classes)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sileshu/anaconda/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         sample_weight = np.asarray([]\n",
      "\u001b[0;32m/Users/sileshu/anaconda/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36m_validate_targets\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m             raise ValueError(\n",
      "\u001b[0;32m/Users/sileshu/anaconda/lib/python2.7/site-packages/sklearn/utils/class_weight.pyc\u001b[0m in \u001b[0;36mcompute_class_weight\u001b[0;34m(class_weight, classes, y)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Class label {} not present.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Class label 2 not present."
     ]
    }
   ],
   "source": [
    "clf_7 = OneVsRestClassifier(SVC(probability=True, class_weight = inter_safety_dic))\n",
    "# cv_results = cross_validate(clf, x_train, y_train, cv=5)\n",
    "clf_7.fit(x_train,encoded_y_train)\n",
    "y_pre_7 = clf_7.predict(x_test)\n",
    "y_pos_7 = clf_7.predict_proba(x_test)\n",
    "\n",
    "print precision_score(encoded_y_test,y_pre_7,average = 'weighted')\n",
    "print precision_score(encoded_y_test,y_pre_7,average = 'micro')\n",
    "print recall_score(encoded_y_test,y_pre_7,average = 'weighted')\n",
    "print recall_score(encoded_y_test,y_pre_7,average = 'micro')\n",
    "print f1_score(encoded_y_test,y_pre_7,average = 'weighted')\n",
    "print f1_score(encoded_y_test,y_pre_7,average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_rf_2 = RandomForestClassifier(class_weight = inter_safety)\n",
    "scores_7 = cross_validate(clf_rf_2, x_total, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_weighted:0.84\n",
      "test_recall_weighted:0.65\n",
      "test_f1_weighted:0.67\n",
      "test_precision_micro:0.88\n",
      "test_recall_micro:0.64\n",
      "test_f1_micro:0.74\n"
     ]
    }
   ],
   "source": [
    "show_results(scores_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.87\n",
      "recall_micro:0.64\n",
      "f1_micro:0.74\n",
      "precision_weighted:0.79\n",
      "recall_weighted:0.65\n",
      "f1_weighted:0.66\n",
      "risk_factor:0.4941\n"
     ]
    }
   ],
   "source": [
    "clf_rf_2 = RandomForestClassifier(class_weight = inter_safety)\n",
    "clf_rf_2.fit(x_train,encoded_y_train)\n",
    "y_pre_8 = clf_rf_2.predict(x_test)\n",
    "y_pos_8 = clf_rf_2.predict_proba(x_test)\n",
    "show_test_results(encoded_y_test, y_pre_8, trans_prob(y_pos_8), class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_dt_2 = DecisionTreeClassifier(class_weight = inter_safety)\n",
    "scores_8 = cross_validate(clf_dt_2, x_total, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_weighted:0.77\n",
      "test_recall_weighted:0.78\n",
      "test_f1_weighted:0.77\n",
      "test_precision_micro:0.79\n",
      "test_recall_micro:0.78\n",
      "test_f1_micro:0.79\n"
     ]
    }
   ],
   "source": [
    "show_results(scores_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.80\n",
      "recall_micro:0.80\n",
      "f1_micro:0.80\n",
      "precision_weighted:0.77\n",
      "recall_weighted:0.80\n",
      "f1_weighted:0.78\n",
      "risk_factor:0.3885\n"
     ]
    }
   ],
   "source": [
    "clf_dt_2 = DecisionTreeClassifier(class_weight = inter_safety)\n",
    "clf_dt_2.fit(x_train,encoded_y_train)\n",
    "y_pre_9 = clf_dt_2.predict(x_test)\n",
    "y_pos_9 = clf_dt_2.predict_proba(x_test)\n",
    "show_test_results(encoded_y_test, y_pre_9, trans_prob(y_pos_9), class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_rf_3 = RandomForestClassifier(class_weight = inter_safety)\n",
    "scores_9 = cross_validate(clf_rf_3, x_total_ngram, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_weighted:0.82\n",
      "test_recall_weighted:0.62\n",
      "test_f1_weighted:0.64\n",
      "test_precision_micro:0.88\n",
      "test_recall_micro:0.61\n",
      "test_f1_micro:0.72\n"
     ]
    }
   ],
   "source": [
    "show_results(scores_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.88\n",
      "recall_micro:0.61\n",
      "f1_micro:0.72\n",
      "precision_weighted:0.76\n",
      "recall_weighted:0.60\n",
      "f1_weighted:0.62\n",
      "risk_factor:0.4644\n"
     ]
    }
   ],
   "source": [
    "clf_rf_3 = RandomForestClassifier(class_weight = inter_safety)\n",
    "clf_rf_3.fit(x_train_ngram,encoded_y_train)\n",
    "y_pre_10 = clf_rf_3.predict(x_test_ngram)\n",
    "y_pos_10 = clf_rf_3.predict_proba(x_test_ngram)\n",
    "show_test_results(encoded_y_test, y_pre_10, trans_prob(y_pos_10), class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_dt_3 = DecisionTreeClassifier(class_weight = inter_safety)\n",
    "scores_10 = cross_validate(clf_dt_3, x_total_ngram, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_weighted:0.71\n",
      "test_recall_weighted:0.72\n",
      "test_f1_weighted:0.71\n",
      "test_precision_micro:0.73\n",
      "test_recall_micro:0.72\n",
      "test_f1_micro:0.73\n"
     ]
    }
   ],
   "source": [
    "show_results(scores_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.75\n",
      "recall_micro:0.73\n",
      "f1_micro:0.74\n",
      "precision_weighted:0.71\n",
      "recall_weighted:0.70\n",
      "f1_weighted:0.70\n",
      "risk_factor:0.5405\n"
     ]
    }
   ],
   "source": [
    "clf_dt_3 = DecisionTreeClassifier(class_weight = inter_safety)\n",
    "clf_dt_3.fit(x_train_ngram,encoded_y_train)\n",
    "y_pre_11 = clf_dt_3.predict(x_test_ngram)\n",
    "y_pos_11 = clf_dt_3.predict_proba(x_test_ngram)\n",
    "show_test_results(encoded_y_test, y_pre_11, trans_prob(y_pos_11), class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.78\n",
      "recall_micro:0.75\n",
      "f1_micro:0.76\n",
      "precision_weighted:0.73\n",
      "recall_weighted:0.72\n",
      "f1_weighted:0.72\n",
      "risk_factor:0.4590\n"
     ]
    }
   ],
   "source": [
    "clf_dt_4 = DecisionTreeClassifier()\n",
    "clf_dt_4.fit(x_train_ngram,encoded_y_train)\n",
    "y_pre_12 = clf_dt_4.predict(x_test_ngram)\n",
    "y_pos_12 = clf_dt_4.predict_proba(x_test_ngram)\n",
    "y_pre_12, y_pos_12 = filtering(y_pre_12, trans_prob(y_pos_12), .11)\n",
    "show_test_results(encoded_y_test, y_pre_12, y_pos_12, class_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.84\n",
      "recall_micro:0.82\n",
      "f1_micro:0.83\n",
      "precision_weighted:0.82\n",
      "recall_weighted:0.80\n",
      "f1_weighted:0.81\n",
      "risk_factor:0.2853\n"
     ]
    }
   ],
   "source": [
    "clf_dt_5 = DecisionTreeClassifier()\n",
    "clf_dt_5.fit(x_train,encoded_y_train)\n",
    "y_pre_13 = clf_dt_5.predict(x_test)\n",
    "y_pos_13 = clf_dt_5.predict_proba(x_test)\n",
    "y_pre_13, y_pos_13 = filtering(y_pre_13, trans_prob(y_pos_13), .11)\n",
    "show_test_results(encoded_y_test, y_pre_13, y_pos_13, class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.88\n",
      "recall_micro:0.65\n",
      "f1_micro:0.75\n",
      "precision_weighted:0.78\n",
      "recall_weighted:0.65\n",
      "f1_weighted:0.66\n",
      "risk_factor:0.4262\n"
     ]
    }
   ],
   "source": [
    "clf_rf_4 = RandomForestClassifier()\n",
    "clf_rf_4.fit(x_train_ngram,encoded_y_train)\n",
    "y_pre_14 = clf_rf_4.predict(x_test_ngram)\n",
    "y_pos_14 = clf_rf_4.predict_proba(x_test_ngram)\n",
    "y_pre_14, y_pos_14 = filtering(y_pre_14, trans_prob(y_pos_14), .11)\n",
    "show_test_results(encoded_y_test, y_pre_14, y_pos_14, class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.90\n",
      "recall_micro:0.72\n",
      "f1_micro:0.80\n",
      "precision_weighted:0.83\n",
      "recall_weighted:0.72\n",
      "f1_weighted:0.74\n",
      "risk_factor:0.3941\n"
     ]
    }
   ],
   "source": [
    "clf_rf_5 = RandomForestClassifier()\n",
    "clf_rf_5.fit(x_train,encoded_y_train)\n",
    "y_pre_15 = clf_rf_5.predict(x_test)\n",
    "y_pos_15 = clf_rf_5.predict_proba(x_test)\n",
    "y_pre_15, y_pos_15 = filtering(y_pre_15, trans_prob(y_pos_15), .11)\n",
    "show_test_results(encoded_y_test, y_pre_15, y_pos_15, class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_weighted:0.64\n",
      "test_recall_weighted:0.64\n",
      "test_f1_weighted:0.62\n",
      "test_precision_micro:0.83\n",
      "test_recall_micro:0.65\n",
      "test_f1_micro:0.73\n"
     ]
    }
   ],
   "source": [
    "# clfs using feature vectors\n",
    "clf_16 = OneVsRestClassifier(SVC(kernel = 'linear', probability=True))\n",
    "scores_16 = cross_validate(clf_16, train_vec, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)\n",
    "show_results(scores_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.81\n",
      "recall_micro:0.64\n",
      "f1_micro:0.72\n",
      "precision_weighted:0.62\n",
      "recall_weighted:0.63\n",
      "f1_weighted:0.61\n",
      "risk_factor:0.4308\n"
     ]
    }
   ],
   "source": [
    "clf_16 = OneVsRestClassifier(SVC(kernel = 'linear', probability=True))\n",
    "clf_16.fit(train_vec, encoded_y_total)\n",
    "y_pre_16 = clf_16.predict(test_vec)\n",
    "y_pos_16 = clf_16.predict_proba(test_vec)\n",
    "show_test_results(encoded_y_test, y_pre_16, y_pos_16, class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_weighted:0.72\n",
      "test_recall_weighted:0.66\n",
      "test_f1_weighted:0.67\n",
      "test_precision_micro:0.80\n",
      "test_recall_micro:0.66\n",
      "test_f1_micro:0.72\n"
     ]
    }
   ],
   "source": [
    "clf_rf_6 = RandomForestClassifier()\n",
    "scores_17 = cross_validate(clf_rf_6, train_vec, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)\n",
    "show_results(scores_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.76\n",
      "recall_micro:0.60\n",
      "f1_micro:0.67\n",
      "precision_weighted:0.65\n",
      "recall_weighted:0.60\n",
      "f1_weighted:0.60\n",
      "risk_factor:0.5956\n"
     ]
    }
   ],
   "source": [
    "clf_rf_6 = RandomForestClassifier()\n",
    "clf_rf_6.fit(train_vec, encoded_y_total)\n",
    "y_pre_17 = clf_rf_6.predict(test_vec)\n",
    "y_pos_17 = clf_rf_6.predict_proba(test_vec)\n",
    "show_test_results(encoded_y_test, y_pre_17, trans_prob(y_pos_17), class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_weighted:0.66\n",
      "test_recall_weighted:0.66\n",
      "test_f1_weighted:0.66\n",
      "test_precision_micro:0.69\n",
      "test_recall_micro:0.66\n",
      "test_f1_micro:0.67\n"
     ]
    }
   ],
   "source": [
    "clf_dt_6 = DecisionTreeClassifier()\n",
    "scores_18 = cross_validate(clf_dt_6, train_vec, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)\n",
    "show_results(scores_18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.64\n",
      "recall_micro:0.61\n",
      "f1_micro:0.63\n",
      "precision_weighted:0.62\n",
      "recall_weighted:0.60\n",
      "f1_weighted:0.60\n",
      "risk_factor:0.8208\n"
     ]
    }
   ],
   "source": [
    "clf_dt_6 = DecisionTreeClassifier()\n",
    "clf_dt_6.fit(train_vec, encoded_y_total)\n",
    "y_pre_18 = clf_dt_6.predict(test_vec)\n",
    "y_pos_18 = clf_dt_6.predict_proba(test_vec)\n",
    "show_test_results(encoded_y_test, y_pre_18, trans_prob(y_pos_18), class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.92\n",
      "recall_micro:0.88\n",
      "f1_micro:0.90\n",
      "precision_weighted:0.88\n",
      "recall_weighted:0.86\n",
      "f1_weighted:0.87\n",
      "risk_factor:0.2294\n"
     ]
    }
   ],
   "source": [
    "clf_19 = OneVsRestClassifier(SVC(kernel = 'linear', probability=True))\n",
    "clf_19.fit(x_train, encoded_y_train)\n",
    "y_pre_19 = clf_19.predict(x_test)\n",
    "y_pos_19 = clf_19.predict_proba(x_test)\n",
    "y_pre_19, y_pos_19 = filtering(y_pre_19, y_pos_19, .11)\n",
    "show_test_results(encoded_y_test, y_pre_19, y_pos_19, class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.89\n",
      "recall_micro:0.77\n",
      "f1_micro:0.83\n",
      "precision_weighted:0.83\n",
      "recall_weighted:0.77\n",
      "f1_weighted:0.78\n",
      "risk_factor:0.3084\n"
     ]
    }
   ],
   "source": [
    "clf_20 = OneVsRestClassifier(SVC(kernel = 'linear', probability=True))\n",
    "clf_20.fit(x_train_ngram, encoded_y_train)\n",
    "y_pre_20 = clf_20.predict(x_test_ngram)\n",
    "y_pos_20 = clf_20.predict_proba(x_test_ngram)\n",
    "y_pre_20, y_pos_20 = filtering(y_pre_20, y_pos_20, .11)\n",
    "show_test_results(encoded_y_test, y_pre_20, y_pos_20, class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.float64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function preprocess at 0x1a509c9b18>,\n",
       "        smooth_idf=True, stop_words='english', strip_accents=None,\n",
       "        sublinear_tf=False, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('unigram_vectorizer.txt', 'w') as uv:\n",
    "    pickle.dump(vectorizer, uv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('linear_svmclf_unigram.txt', 'w') as ls:\n",
    "    pickle.dump(clf, ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('linear_svmclf_feat_vec.txt', 'w') as ls_vec:\n",
    "    pickle.dump(clf_16, ls_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
